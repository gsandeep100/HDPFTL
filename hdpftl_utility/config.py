# Directory Structure (suggested)
# hdpftl/
# ‚îú‚îÄ‚îÄ __init__.py
# ‚îú‚îÄ‚îÄ config.py.py
# ‚îú‚îÄ‚îÄ hdpftl_data/
# ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
# ‚îÇ   ‚îú‚îÄ‚îÄ downloader.py
# ‚îÇ   ‚îî‚îÄ‚îÄ preprocess.py
# ‚îú‚îÄ‚îÄ hdpftl_models/
# ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
# ‚îÇ   ‚îî‚îÄ‚îÄ TabularNet.py
# ‚îú‚îÄ‚îÄ hdpftl_training/
# ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
# ‚îÇ   ‚îú‚îÄ‚îÄ local_train.py
# ‚îÇ   ‚îî‚îÄ‚îÄ federated.py
# ‚îú‚îÄ‚îÄ evaluate.py
# ‚îú‚îÄ‚îÄ hdpftl_main.py
# ‚îî‚îÄ‚îÄ utils.py
import os
from string import Template

from hdpftl_utility.utils import get_today_date

# Main Code Flow Based on the Diagrams
# üîÅ Level 1: Device-Level Training (Personalized Learning)
# Collect device-specific hdpftl_data
#
# Train personalized local hdpftl_models (train_device_model)
#
# Optionally evaluate and fine-tune with historical normal hdpftl_data to detect anomalies
#
# üîÅ Level 2: Fleet-Level Aggregation (Edge Aggregation)
# Send model weights to fleet edge aggregator
#
# Aggregate weights using aggregate_models(hdpftl_models) ‚Üí output: fleet model
#
# Broadcast fleet model back to devices for local adaptation
#
# üîÅ Level 3: Global Aggregation (Cross-Silo Transfer)
# Multiple fleet hdpftl_models (from different silos) are aggregated at the cloud/global level
#
# Use aggregate_models() again ‚Üí output: global model
#
# Optionally fine-tune global model with pre-hdpftl_training learning on target silo's hdpftl_data


# 1. Data Preprocessing per Silo
# Load raw hdpftl_data (e.g., CICIDS2017) for each IoT silo.
#
# Apply:
#
# Label encoding / one-hot encoding
#
# Feature scaling (MinMaxScaler / StandardScaler)
#
# Handle imbalance (SMOTE / ADASYN)
#
# Train/test split
#
# Non-IID partitioning (e.g., Dirichlet distribution)
#
# 2. Base Model Setup
# Load a pretrained model (e.g., ResNet-18, or others)
#
# Modify final layers for intrusion detection classes
#
# Freeze lower layers (for pre-hdpftl_training learning)
#
# 3. Personalized Training per Client (IoT Silo)
# Each silo:
#
# Fine-tunes its local model on local (non-IID) hdpftl_data
#
# Uses pre-hdpftl_training learning to adapt the shared model
#
# Apply regularization to maintain personalization
#
# 4. Local Updates
# Train using FedAvg or FedProx variants
#
# Save local weights
#
# 5. Hierarchical Aggregation
# Edge Level (First Aggregation):
#
# Clients under one regional edge node send updates
#
# Aggregation via weighted average / FedAvg / attention-based fusion
#
# Global Level (Second Aggregation):
#
# Edge nodes send their aggregated hdpftl_models to the cloud
#
# Global model update using hierarchical FedAvg
#
# 6. Model Distribution
# Updated global model pushed down:
#
# To edges
#
# Then to silos (personalized re-init if needed)
#
# 7. Evaluation
# Test each personalized client model on local test set
#
# Log:
#
# Accuracy, Precision, Recall, F1, AUC
#
# Zero-day attack detection performance
#
# 8. Repeat for Several Rounds
# Federated learning is iterative (e.g., 50‚Äì100 rounds)
#
# Optional: dynamic client participation, dropout, model staleness
#
# 9. Final Deployment
# Use the final personalized model per silo
#
# Deployed on-device or on-edge for real-time zero-day detection
INPUT_DATASET_PATH_2023 = 'https://www.unb.ca/cic/datasets/iotdataset-2023.html'
INPUT_DATASET_PATH_2024 = 'https://www.unb.ca/cic/datasets/iotdataset-2024.html'
OUTPUT_DATASET_PATH_2023 = "./hdpftl_dataset/CIC_IoT_IDAD_Dataset_2023/"
OUTPUT_DATASET_PATH_2024 = "./hdpftl_dataset/CIC_IoT_IDAD_Dataset_2024/"
OUTPUT_DATASET_ALL_DATA = "./hdpftl_training/hdpftl_dataset/AllData/"

BATCH_SIZE = 5
NUM_CLIENTS = 1
NUM_DEVICES_PER_CLIENT = 1
# CLIENTS_PER_AGGREGATOR = 5
# NUM_ROUNDS = 10
INPUT_DIM = 79  # Your feature size
NUM_EPOCHS_PRE_TRAIN = 1  # or 50 or 100
NUM_FEDERATED_ROUND = 2  # or 50 or 100
# NUM_TRAIN_ON_DEVICE = 10  # or 50 or 100
NUM_CLASSES = 2  # Suppose you pretrained with 5 classes
GLOBAL_MODEL_PATH_TEMPLATE = Template("./hdpftl_trained_models/${n}/global_model.pth")
FINETUNE_MODEL_PATH_TEMPLATE = Template("./hdpftl_trained_models/${n}/fine_tuned_tabular_model.pth")
PRE_MODEL_FOLDER_PATH_TEMPLATE = Template("./hdpftl_trained_models/${n}/")
PRE_MODEL_PATH_TEMPLATE = Template("./hdpftl_trained_models/${n}/pretrained_tabular_model.pth")
EPOCH_DIR_TEMPLATE = Template("./hdpftl_trained_models/${n}/epochs")
TRAINED_MODEL_DIR = "./hdpftl_trained_models/"
LOGS_DIR_TEMPLATE = Template("./hdpftl_logs/${dataset}/${date}/")
# Substitute to get the string path
EPOCH_DIR = EPOCH_DIR_TEMPLATE.substitute(n=get_today_date())

EPOCH_FILE_FINE = os.path.join(EPOCH_DIR, "fine_tune_epoch_losses.npy")
EPOCH_FILE_PRE = os.path.join(EPOCH_DIR, "pre_epoch_losses.npy")

PLOT_PATH = "./hdpftl_plot_outputs/"
PERSONALISED_MODEL_PATH_TEMPLATE = Template("./hdpftl_trained_models/${n}/personalized_model_client_${n}.pth")
